{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413bf0e2-cd69-4135-b817-e037a9a25fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Integrate Modern Data Architectures with Generative AI and interact using prompts for querying SQL databases & APIs\n",
    "\n",
    "# %% [markdown]\n",
    "# This notebook demonstrates how **_large language models, such as Anthropic,_** interact with AWS databases, data stores, and third-party data warehousing solutions like Snowflake. We showcase this interaction 1) by generating and running SQL queries, and 2) making requests to API endpoints. We achieve all of this by using the LangChain framework, which allows the language model to interact with its environment and connect with other sources of data. The LangChain framework operates based on the following principles: calling out to a language model, being data-aware, and being agentic. Our notebook focuses on establishing database connections to various data sources, consolidating metadata, and returning fact-based data points in response to user queries using LLMs and LangChain.\n",
    "\n",
    "# %%\n",
    "#pip install -r requirements.txt\n",
    "#pip install -U langchain langchain_experimental\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import boto3\n",
    "import streamlit as st\n",
    "import os\n",
    "import sqlalchemy\n",
    "\n",
    "from sql_tools import CustomSQLDatabaseToolkit, CustomRetriever\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "from langchain.agents import AgentType, create_sql_agent\n",
    "\n",
    "# assign your llm and db\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain, SQLDatabaseSequentialChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "#from langchain.chains.sql_database.prompt import PROMPT_SUFFIX, _mysql_prompt\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts.chat import (ChatPromptTemplate)\n",
    "from langchain.prompts import PromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "from typing import Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36467de5-107b-460e-9f7a-d313a1482919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "#Define Glue for \n",
    "\n",
    "# %%\n",
    "\n",
    "glue_crawler_name = '2023_hackathon_ruth_group_finance' #params['CFNCrawlerName'] \n",
    "glue_database_name = '2023_hackathon_sfdc' #params['CFNDatabaseName']\n",
    "glue_databucket_name = 'finance_twitch_finance' #params['DataBucketName']\n",
    "region = 'us-east-1' #outputs['Region']\n",
    "\n",
    "# %% [markdown]\n",
    "# **Important**: The code below establishes a database connection for data sources and Large Language Models. Please note that the solution will only work if the database connection for your sources is defined in the cell below. Please refer to the Pre-requisites section. If your use case requires data from Aurora MySQL alone, then please comment out other data sources. Furthermore, please update the cluster details and variables for Aurora MySQL accordingly.\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "#S3\n",
    "# connect to s3 using athena\n",
    "## athena variables\n",
    "connathena=f\"athena.{region}.amazonaws.com\" \n",
    "portathena='443' #Update, if port is different\n",
    "schemaathena='2023_hackathon_sfdc' #from cfn params\n",
    "s3stagingathena=f's3://2023-ruth-hackathon-group-gen-ai/athenaresults/'#from cfn params\n",
    "wkgrpathena='primary'#Update, if workgroup is different\n",
    "tablesathena=['finance_twitch_finance'] #[<tabe name>]\n",
    "##  Create the athena connection string\n",
    "connection_string = f\"awsathena+rest://@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}/&work_group={wkgrpathena}\"\n",
    "# print(connection_string)\n",
    "##  Create the athena  SQLAlchemy engine\n",
    "engine_athena = create_engine(connection_string, echo=False)\n",
    "dbathena = SQLDatabase(engine_athena)\n",
    "db = dbathena\n",
    "#Glue Data Catalog\n",
    "##Provide list of all the databases where the table metadata resides after the glue successfully crawls the table\n",
    "# gdc = ['redshift-sagemaker-sample-data-dev', 'snowflake','rds-aurora-mysql-employees','sagemaker_featurestore'] # mentioned a few examples here\n",
    "gdc = [schemaathena] \n",
    "# print(gdc)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Step 2 - Generate Dynamic Prompt Templates\n",
    "# Build a consolidated view of Glue Data Catalog by combining metadata stored for all the databases in pipe delimited format.\n",
    "\n",
    "# %%\n",
    "#Generate Dynamic prompts to populate the Glue Data Catalog\n",
    "#harvest aws crawler metadata\n",
    "\n",
    "def parse_catalog():\n",
    "    #Connect to Glue catalog\n",
    "    #get metadata of redshift serverless tables\n",
    "    columns_str=''\n",
    "    #define glue cient\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    for db in gdc:\n",
    "        response = glue_client.get_tables(DatabaseName =db)\n",
    "        #return response\n",
    "        for tables in response['TableList']:\n",
    "            for columns in tables['StorageDescriptor']['Columns']:\n",
    "                    dbname,tblname,colname=tables['DatabaseName'],tables['Name'],columns['Name']\n",
    "                    columns_str=columns_str+f'{dbname}|{tblname}|{colname}\\n'                     \n",
    "    #API\n",
    "    ## Append the metadata of the API to the unified glue data catalog\n",
    "    columns_str=columns_str+'\\n '\n",
    "    return columns_str\n",
    "\n",
    "glue_catalog = parse_catalog()\n",
    "\n",
    "# print(glue_catalog)\n",
    "# display a few lines from the catalog\n",
    "# print('\\n'.join(glue_catalog.splitlines()[-10:]) )\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "### Step 3 - Define Functions to 1/ determine the best data channel to answer the user query, 2/ Generate response to  user query\n",
    "\n",
    "# %%\n",
    "#In this code sample, we use the Anthropic Model to generate inferences. You can utilize SageMaker JumpStart models  to achieve the same. \n",
    "#Guidance on how to use the JumpStart Models is available in the notebook - mda_with_llm_langchain_smjumpstart_flant5xl\n",
    "\n",
    "# %%\n",
    "\n",
    "#INITIALIZE BEDROCK CLIENT\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "#Memory\n",
    "# \n",
    "# memory = ConversationBufferMemory()\n",
    "# memory.chat_memory.add_user_message(\"You will act as a principal business analyst and answer business analyst questions.\")\n",
    "# memory.chat_memory.add_ai_message(\"I am a principal business analyst will answer your analytics questions.\")\n",
    "#LLM \n",
    "#llm = Bedrock(model_id=\"amazon.titan-text-express-v1\")\n",
    "#llm = Bedrock(model_id = \"anthropic.claude-v2\")\n",
    "llm = Bedrock(model_id=\"ai21.j2-ultra-v1\", model_kwargs={\"maxTokens\": 1024,\"temperature\": 0.0})\n",
    "\n",
    "# conversation = ConversationChain(\n",
    "#      llm=llm, verbose=True, memory=memory\n",
    "# )\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "\n",
    "\n",
    "def identify_table(query):\n",
    "    prompt_template = \"\"\"\n",
    "    From the table below, find the database (in column database) which will contain the data (in corresponding column_names) to answer the question {query}\n",
    "    \"\"\"+glue_catalog+\"\"\"Give your answer as database == \\n Also,give your answer as database.table == \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"query\"])\n",
    "    # define llm chain\n",
    "    llm_chain = LLMChain(prompt=PROMPT, llm=llm)\n",
    "    generated_texts = llm_chain.run(query)\n",
    "    return generated_texts\n",
    "\n",
    "#def run_query(query):\n",
    "##    query_tables = identify_table(query)\n",
    "#    _DEFAULT_TEMPLATE = \"\"\"\n",
    "#    Create a syntactically correct athena query to run based on the question\n",
    "#    \n",
    "#    First retrieve the table name, table columns\n",
    "#    Then check the query to ensure all columns are part of their respective tables.\n",
    "#    Then check the query to ensure it follows correct sql syntax.\n",
    "#    Use only the finance_twitch_finance\n",
    "#    {table_info}\n",
    "#    \n",
    "#    Question: {input}\n",
    "#   \n",
    "#    Context:\n",
    "#    Use local currency column (ie. lc_reported_revenue) if question requests for currency\n",
    "#    Use usd_reported_revenue if question requets usd as currency\n",
    "#    the column report_year refers to the year the revenue was recorded\n",
    "#    the column report_month refers to the month the revenue was recorded    \n",
    "#    \n",
    "#    Respond to the answer in a human readable sentence, if using numbers format to the nearest whole number\n",
    "#           \"\"\"\n",
    "#    PROMPT = PromptTemplate(template=_DEFAULT_TEMPLATE, input_variables=[\"input\",\"table_info\"])\n",
    "#    db = dbathena    \n",
    "#    db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose = True,\n",
    "#                                         use_query_checker=True, # self-correcting small mistakes NOT WORKING\n",
    "#                                         top_k=3 # limit the number of rows returned\n",
    "#                                        )\n",
    "#    response=db_chain.run(query)\n",
    "#    return response\n",
    "#\n",
    "\n",
    "\n",
    "# %%\n",
    "# In This Code we are testing SQL Database Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7ff74d-24b3-4022-a8bc-ac287bd02c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:13:49.554 INFO    sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-12-04 10:13:50.781 INFO    sentence_transformers.SentenceTransformer: Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d83c4b34874472c8634f08c783daa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Which Account in the Hardlines sales vertical reported the most total revenue in 2023?\n",
      "SQLQuery:"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ccf3e4fe7340cbacd9403c9e44ca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mSELECT sales_vertical ,  salesforce_account_name ,  sum(usd_reported_revenue) as revenue FROM finance_twitch_finance WHERE report_year = 2023 AND sales_vertical  =  'Hardlines' GROUP BY 1,2 ORDER BY revenue DESC LIMIT 1;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Hardlines', 'Samsung - US', 245084053.53340006)]\u001b[0m\n",
      "Answer:"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114a991587854633b1022b9cbd6217c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mSamsung - US reported the most total revenue in 2023 in the Hardlines sales vertical with $245,084,053.53340006\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Samsung - US reported the most total revenue in 2023 in the Hardlines sales vertical with $245,084,053.53340006'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#sql_toolkit = SQLDatabaseToolkit(db=db, llm=llm , use_query_checker=True)\n",
    "sql_toolkit = CustomSQLDatabaseToolkit(db=db, llm=llm , use_query_checker=True)\n",
    "sql_toolkit.get_tools()\n",
    "\n",
    "sqldb_agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit = sql_toolkit,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "#    memory=memory\n",
    ")\n",
    "\n",
    "examples = [\n",
    "        {\n",
    "            \"input\": \"Which account has the highest revenue in 2023?\",\n",
    "            \"sql_cmd\": \"\"\"SELECT salesforce_account_name, sum(usd_reported_revenue) as revenue \n",
    "                          FROM finance_twitch_finance\n",
    "                          Where report_year = 2023\n",
    "                          Group By 1\n",
    "                          Order by 2 desc\n",
    "                          Limit 1\n",
    "                    ;\"\"\",\n",
    "            \"result\": \"[Apple - Global, (100000000,)]\",\n",
    "            \"answer\": \"Apple - Global has an 2023 annual revenue of $100,000,000\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Which account has the highest revenue in the entertainment sales_vertical in 2023?\",\n",
    "            \"sql_cmd\":\"\"\"SELECT sales_vertical, salesforce_account_name, sum(usd_reported_revenue) as revenue \n",
    "                          FROM finance_twitch_finance\n",
    "                          Where report_year = 2023 and sales_vertical = 'Entertainment'\n",
    "                          Group By 1,2\n",
    "                          Order by 2 desc\n",
    "                          Limit 1\n",
    "                    ;\"\"\",\n",
    "            \"result\": \"[Entertainment, Apple - Global, (100000000,)]\",\n",
    "            \"answer\": \"Apple - Global has the highest 2023 annual revenue in the Entertainment sales_vertical with $100,000,000\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Which product did Apple - Global spend the most revenue on in 2023\",\n",
    "            \"sql_cmd\":\"\"\"SELECT salesforce_account_name, property_name, sum(usd_reported_revenue) as revenue \n",
    "                          FROM finance_twitch_finance\n",
    "                          Where report_year = 2023 and salesforce_account_name = 'Apple - Global'\n",
    "                          Group By 1,2\n",
    "                          Order by 3 desc\n",
    "                          Limit 1\n",
    "                    ;\"\"\",\n",
    "            \"result\": \"[Audio Ads, (30000,)]\",\n",
    "            \"answer\": \"Apple - Global spent the most revenue on 'Audio Ads' with a 2023 annual spend of $30,000\",\n",
    "        },\n",
    "]\n",
    "\n",
    "_mysql_prompt_ = \"\"\"\n",
    "You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\"\"\"\n",
    "\n",
    "#example_prompt = PromptTemplate(\n",
    "#    input_variables=[\"input\", \"sql_cmd\", \"result\", \"answer\",],\n",
    "#    template=\"\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {result}\\nAnswer: {answer}\",\n",
    "#)\n",
    "\n",
    "#example_prompt = ChatPromptTemplate.from_messages(\n",
    "#    [\n",
    "#        (\"human\", \"{input}\"),\n",
    "#        (\"ai\", \"\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {result}\\nAnswer: {answer}\"),\n",
    "#    ]\n",
    "#)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"sql_cmd\", \"result\", \"answer\",],\n",
    "    template=\"\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {result}\\nAnswer: {answer}\",\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "\n",
    "#few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "#    example_selector=example_selector,\n",
    "#    example_prompt=example_prompt,\n",
    "#    #prefix=_mysql_prompt_,\n",
    "#    #suffix=\"Question: {input}\",\n",
    "#    #suffix=prompt_suffix, \n",
    "#    input_variables=[\"input\"], #These variables are used in the prefix and suffix\n",
    "#)\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=_mysql_prompt_,\n",
    "    suffix=\"Question: {input}\",\n",
    "    #suffix=prompt_suffix, \n",
    "    input_variables=[\"input\"], #These variables are used in the prefix and suffix\n",
    ")\n",
    "\n",
    "#final_prompt = ChatPromptTemplate.from_messages(\n",
    "#    [ (\"system\", _mysql_prompt_),\n",
    "#     few_shot_prompt,\n",
    "#      (\"human\", \"{input}\"),\n",
    "#    ]\n",
    "#)\n",
    "#\n",
    "# Run Queries\n",
    "query_01 = \"\"\"Which account has the highest revenue in 2023?\"\"\" \n",
    "query_02 = \"\"\"Which Account in the Hardlines sales vertical reported the most total revenue in 2023?\"\"\" \n",
    "query_03 = \"\"\"Can you compare top 2 unique account by total revenue?\"\"\"\n",
    "\n",
    "local_chain = SQLDatabaseChain.from_llm(llm, db, prompt=few_shot_prompt, use_query_checker=True, \n",
    "                                        verbose=True, return_sql=False,)\n",
    "local_chain.run(query_02)\n",
    "\n",
    "#sqldb_agent.run(final_prompt.format(question=query_01 ))\n",
    "#print(sqldb_agent.run(few_shot_prompt.format(input=query_01)))\n",
    "#print(final_prompt.format(input=query_01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b3db4-3275-4da2-945e-2883de447928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
